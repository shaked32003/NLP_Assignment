{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b3791ff",
   "metadata": {},
   "source": [
    "# NLP Assignment\n",
    "This notebook provides hands-on practice with text processing techniques in Python, including tokenization, lemmatization, and stemming. It also includes loading, analyzing, and scraping textual data from different sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320bbad7",
   "metadata": {},
   "source": [
    "## Setup\n",
    "1. Install a Python programming environment (e.g., PyCharm, Jupyter Notebook).\n",
    "2. Install the necessary Python libraries: `nltk`, `spaCy`, `BeautifulSoup`.\n",
    "\n",
    "```python\n",
    "!pip install nltk spacy beautifulsoup4\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import spacy\n",
    "spacy.cli.download('en_core_web_sm')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83e47a",
   "metadata": {},
   "source": [
    "## Data Loading & Basic Analysis\n",
    "First, let's load the dataset and print the required basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536fbac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "# Load the spam.csv dataset\n",
    "file_path = 'spam.csv'  # Replace with your actual file path\n",
    "spam_df = pd.read_csv(file_path, encoding='latin-1')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "spam_df = spam_df[['v1', 'v2']]\n",
    "spam_df.columns = ['label', 'message']\n",
    "\n",
    "# Total number of SMS messages\n",
    "total_messages = len(spam_df)\n",
    "\n",
    "# Number of spam/ham messages\n",
    "spam_messages = len(spam_df[spam_df['label'] == 'spam'])\n",
    "ham_messages = len(spam_df[spam_df['label'] == 'ham'])\n",
    "\n",
    "# Average number of words per message\n",
    "spam_df['word_count'] = spam_df['message'].apply(lambda x: len(word_tokenize(x)))\n",
    "average_word_count = spam_df['word_count'].mean()\n",
    "\n",
    "# Most frequent words\n",
    "all_words = ' '.join(spam_df['message']).lower()\n",
    "all_words_tokenized = word_tokenize(all_words)\n",
    "word_freq = Counter(all_words_tokenized)\n",
    "most_common_words = word_freq.most_common(5)\n",
    "\n",
    "# Number of words that only appear once\n",
    "unique_words_count = sum(1 for word, count in word_freq.items() if count == 1)\n",
    "\n",
    "# Print the statistics\n",
    "print(f\"Total number of SMS messages: {total_messages}\")\n",
    "print(f\"Number of spam messages: {spam_messages}\")\n",
    "print(f\"Number of ham messages: {ham_messages}\")\n",
    "print(f\"Average number of words per message: {average_word_count}\")\n",
    "print(f\"5 most frequent words: {most_common_words}\")\n",
    "print(f\"Number of words that only appear once: {unique_words_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef11dd",
   "metadata": {},
   "source": [
    "## Text Processing\n",
    "Now, let's perform tokenization, lemmatization, and stemming using both `nltk` and `spaCy`.\n",
    "\n",
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45973fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for text processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from time import time\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# NLTK Tokenization\n",
    "start_time = time()\n",
    "spam_df['nltk_tokens'] = spam_df['message'].apply(word_tokenize)\n",
    "nltk_tokenization_time = time() - start_time\n",
    "\n",
    "# spaCy Tokenization\n",
    "start_time = time()\n",
    "spam_df['spacy_tokens'] = spam_df['message'].apply(lambda x: [token.text for token in nlp(x)])\n",
    "spacy_tokenization_time = time() - start_time\n",
    "\n",
    "# Compare tokenization times\n",
    "print(f\"NLTK Tokenization Time: {nltk_tokenization_time} seconds\")\n",
    "print(f\"spaCy Tokenization Time: {spacy_tokenization_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de6e64",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6776637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize lemmatizers\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# NLTK Lemmatization\n",
    "start_time = time()\n",
    "spam_df['nltk_lemmas'] = spam_df['nltk_tokens'].apply(lambda tokens: [nltk_lemmatizer.lemmatize(token) for token in tokens])\n",
    "nltk_lemmatization_time = time() - start_time\n",
    "\n",
    "# spaCy Lemmatization\n",
    "start_time = time()\n",
    "spam_df['spacy_lemmas'] = spam_df['spacy_tokens'].apply(lambda tokens: [token.lemma_ for token in nlp(' '.join(tokens))])\n",
    "spacy_lemmatization_time = time() - start_time\n",
    "\n",
    "# Compare lemmatization times\n",
    "print(f\"NLTK Lemmatization Time: {nltk_lemmatization_time} seconds\")\n",
    "print(f\"spaCy Lemmatization Time: {spacy_lemmatization_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17379c24",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b3da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize stemmers\n",
    "nltk_stemmer = PorterStemmer()\n",
    "\n",
    "# NLTK Stemming\n",
    "start_time = time()\n",
    "spam_df['nltk_stems'] = spam_df['nltk_tokens'].apply(lambda tokens: [nltk_stemmer.stem(token) for token in tokens])\n",
    "nltk_stemming_time = time() - start_time\n",
    "\n",
    "# Compare stemming times\n",
    "print(f\"NLTK Stemming Time: {nltk_stemming_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14461a87",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a7082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of nltk and spaCy implementations\n",
    "\n",
    "print(\"Tokenization Comparison:\")\n",
    "print(\"NLTK produces tokens as a list of strings while spaCy produces tokens as token objects with rich attributes.\")\n",
    "\n",
    "print(\"Lemmatization Comparison:\")\n",
    "print(\"NLTK lemmatizer is simpler and language-agnostic but less powerful than spaCy's lemmatizer which handles context and part-of-speech better.\")\n",
    "\n",
    "print(\"Stemming Comparison:\")\n",
    "print(\"NLTK has a built-in stemmer, whereas spaCy does not include stemming as it focuses on more advanced NLP features.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab52005e",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333922ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping using BeautifulSoup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of a public profile \n",
    "url = 'https://example.com/profile'  # Replace with The url of the person checking\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract text content from the page\n",
    "profile_text = soup.get_text()\n",
    "\n",
    "# Tokenization, Lemmatization, and Stemming of the scraped text\n",
    "profile_tokens = word_tokenize(profile_text)\n",
    "profile_lemmas = [nltk_lemmatizer.lemmatize(token) for token in profile_tokens]\n",
    "profile_stems = [nltk_stemmer.stem(token) for token in profile_tokens]\n",
    "\n",
    "# Print word statistics\n",
    "profile_word_freq = Counter(profile_tokens)\n",
    "print(f\"Profile Text Word Count: {len(profile_tokens)}\")\n",
    "print(f\"Profile Text Most Common Words: {profile_word_freq.most_common(5)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349cbb88",
   "metadata": {},
   "source": [
    "## WhatsApp Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944bf5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a .txt file of WhatsApp messages\n",
    "whatsapp_file_path = 'path/to/whatsapp/messages.txt'  # Replace with actual file path of the person checking\n",
    "\n",
    "# Load the WhatsApp messages\n",
    "with open(whatsapp_file_path, 'r', encoding='utf-8') as file:\n",
    "    whatsapp_text = file.read()\n",
    "\n",
    "# Tokenization, Lemmatization, and Stemming of the WhatsApp messages\n",
    "whatsapp_tokens = word_tokenize(whatsapp_text)\n",
    "whatsapp_lemmas = [nltk_lemmatizer.lemmatize(token) for token in whatsapp_tokens]\n",
    "whatsapp_stems = [nltk_stemmer.stem(token) for token in whatsapp_tokens]\n",
    "\n",
    "# Print word statistics\n",
    "whatsapp_word_freq = Counter(whatsapp_tokens)\n",
    "print(f\"WhatsApp Messages Word Count: {len(whatsapp_tokens)}\")\n",
    "print(f\"WhatsApp Messages Most Common Words: {whatsapp_word_freq.most_common(5)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1d54d6",
   "metadata": {},
   "source": [
    "\n",
    "## Tokenization\n",
    "In this section, we will apply different tokenization methods including white space tokenizer, regex tokenizer, word tokenizer, and sentence tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ca758",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenization Section\n",
    "\n",
    "# White Space Tokenizer\n",
    "def whitespace_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "spam_df['whitespace_tokens'] = spam_df['message'].apply(whitespace_tokenizer)\n",
    "\n",
    "# Regex Tokenizer\n",
    "import re\n",
    "\n",
    "def regex_tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "spam_df['regex_tokens'] = spam_df['message'].apply(regex_tokenizer)\n",
    "\n",
    "# Word Tokenizer (NLTK)\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "spam_df['word_tokens'] = spam_df['message'].apply(word_tokenize)\n",
    "\n",
    "# Sentence Tokenizer (NLTK)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "spam_df['sentence_tokens'] = spam_df['message'].apply(sent_tokenize)\n",
    "\n",
    "# Display tokenized results\n",
    "print(spam_df[['message', 'whitespace_tokens', 'regex_tokens', 'word_tokens', 'sentence_tokens']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3accc0c4",
   "metadata": {},
   "source": [
    "\n",
    "## Normalization\n",
    "In this section, we will apply normalization techniques including stemming and lemmatization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac0d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalization Section\n",
    "\n",
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "spam_df['stemmed_tokens'] = spam_df['word_tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "\n",
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "spam_df['lemmatized_tokens'] = spam_df['word_tokens'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "\n",
    "# Display normalization results\n",
    "print(spam_df[['message', 'stemmed_tokens', 'lemmatized_tokens']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4090e7a1",
   "metadata": {},
   "source": [
    "\n",
    "## Stop Words Removal\n",
    "In this section, we will remove stop words including conjunctions and articles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084b1434",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stop Words Removal Section\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "spam_df['tokens_no_stopwords'] = spam_df['word_tokens'].apply(remove_stopwords)\n",
    "\n",
    "# Display results after stop words removal\n",
    "print(spam_df[['message', 'tokens_no_stopwords']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e652d",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Extraction\n",
    "In this section, we will apply feature extraction techniques including Bag of Words (BOW), TF-IDF, and Word2Vec.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab56e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature Extraction Section\n",
    "\n",
    "# Bag of Words (BOW)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(spam_df['message'])\n",
    "\n",
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(spam_df['message'])\n",
    "\n",
    "# Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [word_tokenize(message) for message in spam_df['message']]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "word2vec_vectors = word2vec_model.wv\n",
    "\n",
    "# Display feature extraction results\n",
    "print(\"Bag of Words shape:\", X_bow.shape)\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)\n",
    "print(\"Word2Vec vocab size:\", len(word2vec_vectors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77105563",
   "metadata": {},
   "source": [
    "\n",
    "## GloVe Explanation and Application\n",
    "GloVe (Global Vectors for Word Representation) is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "\n",
    "To apply GloVe to our data, we need to download pre-trained GloVe embeddings and map our words to these vectors. Since GloVe embeddings are pre-trained, they offer richer semantic meaning than training a Word2Vec model on a small corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fbf448",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_embeddings = {}\n",
    "with open(\"glove.6B.100d.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "        glove_embeddings[word] = vector\n",
    "\n",
    "# Function to get GloVe vector for a word\n",
    "def get_glove_vector(word):\n",
    "    return glove_embeddings.get(word, np.zeros(100))\n",
    "\n",
    "# Applying GloVe to our dataset\n",
    "spam_df['glove_vectors'] = spam_df['tokens_no_stopwords'].apply(lambda tokens: [get_glove_vector(token) for token in tokens])\n",
    "\n",
    "# Display GloVe vector results\n",
    "print(spam_df[['message', 'glove_vectors']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae8e47",
   "metadata": {},
   "source": [
    "\n",
    "## CYK Tagging\n",
    "In this section, we will select 5 sentences and apply tagging using the CYK algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f0bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CYK Tagging Section\n",
    "# Select 5 sentences for CYK tagging\n",
    "sample_sentences = spam_df['message'].sample(5).tolist()\n",
    "\n",
    "# Example CYK tagging (simplified version)\n",
    "# Here we use a dummy grammar for demonstration; replace with an actual grammar for real use cases\n",
    "grammar = {\n",
    "    'S': [['NP', 'VP']],\n",
    "    'NP': [['Det', 'N']],\n",
    "    'VP': [['V', 'NP']],\n",
    "    'Det': ['a', 'the'],\n",
    "    'N': ['dog', 'cat'],\n",
    "    'V': ['chased', 'sat']\n",
    "}\n",
    "\n",
    "def cyk_parse(sentence, grammar):\n",
    "    words = sentence.split()\n",
    "    n = len(words)\n",
    "    table = [[set() for _ in range(n)] for _ in range(n)]\n",
    "    \n",
    "    for j in range(n):\n",
    "        for lhs, rhs in grammar.items():\n",
    "            if words[j] in rhs:\n",
    "                table[j][j].add(lhs)\n",
    "        for i in range(j-1, -1, -1):\n",
    "            for k in range(i, j):\n",
    "                for lhs, rhs in grammar.items():\n",
    "                    if len(rhs) == 2 and rhs[0] in table[i][k] and rhs[1] in table[k+1][j]:\n",
    "                        table[i][j].add(lhs)\n",
    "    \n",
    "    return table\n",
    "\n",
    "# Apply CYK to sample sentences\n",
    "cyk_results = [cyk_parse(sentence, grammar) for sentence in sample_sentences]\n",
    "\n",
    "# Display CYK results\n",
    "for sentence, result in zip(sample_sentences, cyk_results):\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"CYK Parse Table: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf40550",
   "metadata": {},
   "source": [
    "## Adding RNN and LSTM models for next-word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626de74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN and LSTM Models for Next-Word Prediction\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare the data for the RNN and LSTM models\n",
    "def prepare_data(texts, max_sequence_len):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    input_sequences = []\n",
    "    for line in texts:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "    predictors, label = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "    label = np.eye(total_words)[label]\n",
    "    return predictors, label, max_sequence_len, total_words, tokenizer\n",
    "\n",
    "texts = spam_df['message'].tolist()\n",
    "predictors, label, max_sequence_len, total_words, tokenizer = prepare_data(texts, max_sequence_len=20)\n",
    "\n",
    "# Build and train the RNN model\n",
    "def build_rnn_model(max_sequence_len, total_words):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
    "    model.add(SimpleRNN(100))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "rnn_model = build_rnn_model(max_sequence_len, total_words)\n",
    "rnn_model.fit(predictors, label, epochs=20, verbose=1)\n",
    "\n",
    "# Build and train the LSTM model\n",
    "def build_lstm_model(max_sequence_len, total_words):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "lstm_model = build_lstm_model(max_sequence_len, total_words)\n",
    "lstm_model.fit(predictors, label, epochs=20, verbose=1)\n",
    "\n",
    "# Compare the models' performance\n",
    "def evaluate_model(model, tokenizer, seed_text, max_sequence_len):\n",
    "    for _ in range(10):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_word = tokenizer.index_word[np.argmax(predicted)]\n",
    "        seed_text += \" \" + predicted_word\n",
    "    return seed_text\n",
    "\n",
    "seed_text = \"Hello\"\n",
    "print(\"RNN Model Prediction:\", evaluate_model(rnn_model, tokenizer, seed_text, max_sequence_len))\n",
    "print(\"LSTM Model Prediction:\", evaluate_model(lstm_model, tokenizer, seed_text, max_sequence_len))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325a5b01",
   "metadata": {},
   "source": [
    "## Adding Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943190c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Summarization using KL-Sum\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "def kl_summarization(text, top_n=5):\n",
    "    sentences = sent_tokenize(text)\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
    "    sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "    \n",
    "    summary = \" \".join([ranked_sentences[i][1] for i in range(min(top_n, len(ranked_sentences)))])\n",
    "    return summary\n",
    "\n",
    "text = \" \".join(spam_df['message'].tolist())\n",
    "print(\"KL-Sum Summary:\")\n",
    "print(kl_summarization(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a78410",
   "metadata": {},
   "source": [
    "## Fine-Tuning GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959ecbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tuning GPT-2\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Prepare dataset for GPT-2\n",
    "def prepare_gpt2_data(texts, tokenizer, block_size=128):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=\"/mnt/data/spam.csv\",\n",
    "        block_size=block_size,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "dataset = prepare_gpt2_data(texts, tokenizer)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Generate completions for partial sentences\n",
    "def generate_completion(model, tokenizer, seed_text, max_length=50):\n",
    "    input_ids = tokenizer.encode(seed_text, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "partial_sentences = [\n",
    "    \"I am looking forward to\",\n",
    "    \"The weather today is\",\n",
    "    \"Our project deadline is\",\n",
    "    \"The meeting was\",\n",
    "    \"He said that\"\n",
    "]\n",
    "\n",
    "for sentence in partial_sentences:\n",
    "    print(\"Seed text:\", sentence)\n",
    "    print(\"Completion:\", generate_completion(model, tokenizer, sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9785b4a0",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7108300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    return sia.polarity_scores(text)\n",
    "\n",
    "spam_df['sentiment'] = spam_df['message'].apply(analyze_sentiment)\n",
    "spam_df['sentiment_category'] = spam_df['sentiment'].apply(lambda x: 'positive' if x['compound'] > 0 else ('negative' if x['compound'] < 0 else 'neutral'))\n",
    "\n",
    "sentiment_distribution = spam_df['sentiment_category'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Sentiment Distribution:\")\n",
    "print(sentiment_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc0afb",
   "metadata": {},
   "source": [
    "## Adding RNN and LSTM Models for Next-Word Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444759a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN and LSTM Models for Next-Word Prediction\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare the data for the RNN and LSTM models\n",
    "def prepare_data(texts, max_sequence_len):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    input_sequences = []\n",
    "    for line in texts:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "    predictors, label = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "    label = np.eye(total_words)[label]\n",
    "    return predictors, label, max_sequence_len, total_words, tokenizer\n",
    "\n",
    "texts = spam_df['message'].tolist()\n",
    "predictors, label, max_sequence_len, total_words, tokenizer = prepare_data(texts, max_sequence_len=20)\n",
    "\n",
    "# Build and train the RNN model\n",
    "def build_rnn_model(max_sequence_len, total_words):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
    "    model.add(SimpleRNN(100))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
