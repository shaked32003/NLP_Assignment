{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b3791ff",
   "metadata": {},
   "source": [
    "# NLP Assignment\n",
    "This notebook provides hands-on practice with text processing techniques in Python, including tokenization, lemmatization, and stemming. It also includes loading, analyzing, and scraping textual data from different sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320bbad7",
   "metadata": {},
   "source": [
    "## Setup\n",
    "1. Install a Python programming environment (e.g., PyCharm, Jupyter Notebook).\n",
    "2. Install the necessary Python libraries: `nltk`, `spaCy`, `BeautifulSoup`.\n",
    "\n",
    "```python\n",
    "!pip install nltk spacy beautifulsoup4\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import spacy\n",
    "spacy.cli.download('en_core_web_sm')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83e47a",
   "metadata": {},
   "source": [
    "## Data Loading & Basic Analysis\n",
    "First, let's load the dataset and print the required basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536fbac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "# Load the spam.csv dataset\n",
    "file_path = 'spam.csv'  # Replace with your actual file path\n",
    "spam_df = pd.read_csv(file_path, encoding='latin-1')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "spam_df = spam_df[['v1', 'v2']]\n",
    "spam_df.columns = ['label', 'message']\n",
    "\n",
    "# Total number of SMS messages\n",
    "total_messages = len(spam_df)\n",
    "\n",
    "# Number of spam/ham messages\n",
    "spam_messages = len(spam_df[spam_df['label'] == 'spam'])\n",
    "ham_messages = len(spam_df[spam_df['label'] == 'ham'])\n",
    "\n",
    "# Average number of words per message\n",
    "spam_df['word_count'] = spam_df['message'].apply(lambda x: len(word_tokenize(x)))\n",
    "average_word_count = spam_df['word_count'].mean()\n",
    "\n",
    "# Most frequent words\n",
    "all_words = ' '.join(spam_df['message']).lower()\n",
    "all_words_tokenized = word_tokenize(all_words)\n",
    "word_freq = Counter(all_words_tokenized)\n",
    "most_common_words = word_freq.most_common(5)\n",
    "\n",
    "# Number of words that only appear once\n",
    "unique_words_count = sum(1 for word, count in word_freq.items() if count == 1)\n",
    "\n",
    "# Print the statistics\n",
    "print(f\"Total number of SMS messages: {total_messages}\")\n",
    "print(f\"Number of spam messages: {spam_messages}\")\n",
    "print(f\"Number of ham messages: {ham_messages}\")\n",
    "print(f\"Average number of words per message: {average_word_count}\")\n",
    "print(f\"5 most frequent words: {most_common_words}\")\n",
    "print(f\"Number of words that only appear once: {unique_words_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef11dd",
   "metadata": {},
   "source": [
    "## Text Processing\n",
    "Now, let's perform tokenization, lemmatization, and stemming using both `nltk` and `spaCy`.\n",
    "\n",
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45973fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for text processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from time import time\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# NLTK Tokenization\n",
    "start_time = time()\n",
    "spam_df['nltk_tokens'] = spam_df['message'].apply(word_tokenize)\n",
    "nltk_tokenization_time = time() - start_time\n",
    "\n",
    "# spaCy Tokenization\n",
    "start_time = time()\n",
    "spam_df['spacy_tokens'] = spam_df['message'].apply(lambda x: [token.text for token in nlp(x)])\n",
    "spacy_tokenization_time = time() - start_time\n",
    "\n",
    "# Compare tokenization times\n",
    "print(f\"NLTK Tokenization Time: {nltk_tokenization_time} seconds\")\n",
    "print(f\"spaCy Tokenization Time: {spacy_tokenization_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de6e64",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6776637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize lemmatizers\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# NLTK Lemmatization\n",
    "start_time = time()\n",
    "spam_df['nltk_lemmas'] = spam_df['nltk_tokens'].apply(lambda tokens: [nltk_lemmatizer.lemmatize(token) for token in tokens])\n",
    "nltk_lemmatization_time = time() - start_time\n",
    "\n",
    "# spaCy Lemmatization\n",
    "start_time = time()\n",
    "spam_df['spacy_lemmas'] = spam_df['spacy_tokens'].apply(lambda tokens: [token.lemma_ for token in nlp(' '.join(tokens))])\n",
    "spacy_lemmatization_time = time() - start_time\n",
    "\n",
    "# Compare lemmatization times\n",
    "print(f\"NLTK Lemmatization Time: {nltk_lemmatization_time} seconds\")\n",
    "print(f\"spaCy Lemmatization Time: {spacy_lemmatization_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17379c24",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b3da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize stemmers\n",
    "nltk_stemmer = PorterStemmer()\n",
    "\n",
    "# NLTK Stemming\n",
    "start_time = time()\n",
    "spam_df['nltk_stems'] = spam_df['nltk_tokens'].apply(lambda tokens: [nltk_stemmer.stem(token) for token in tokens])\n",
    "nltk_stemming_time = time() - start_time\n",
    "\n",
    "# Compare stemming times\n",
    "print(f\"NLTK Stemming Time: {nltk_stemming_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14461a87",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a7082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of nltk and spaCy implementations\n",
    "\n",
    "print(\"Tokenization Comparison:\")\n",
    "print(\"NLTK produces tokens as a list of strings while spaCy produces tokens as token objects with rich attributes.\")\n",
    "\n",
    "print(\"Lemmatization Comparison:\")\n",
    "print(\"NLTK lemmatizer is simpler and language-agnostic but less powerful than spaCy's lemmatizer which handles context and part-of-speech better.\")\n",
    "\n",
    "print(\"Stemming Comparison:\")\n",
    "print(\"NLTK has a built-in stemmer, whereas spaCy does not include stemming as it focuses on more advanced NLP features.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab52005e",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333922ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping using BeautifulSoup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of a public profile \n",
    "url = 'https://example.com/profile'  # Replace with The url of the person checking\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract text content from the page\n",
    "profile_text = soup.get_text()\n",
    "\n",
    "# Tokenization, Lemmatization, and Stemming of the scraped text\n",
    "profile_tokens = word_tokenize(profile_text)\n",
    "profile_lemmas = [nltk_lemmatizer.lemmatize(token) for token in profile_tokens]\n",
    "profile_stems = [nltk_stemmer.stem(token) for token in profile_tokens]\n",
    "\n",
    "# Print word statistics\n",
    "profile_word_freq = Counter(profile_tokens)\n",
    "print(f\"Profile Text Word Count: {len(profile_tokens)}\")\n",
    "print(f\"Profile Text Most Common Words: {profile_word_freq.most_common(5)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349cbb88",
   "metadata": {},
   "source": [
    "## WhatsApp Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944bf5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a .txt file of WhatsApp messages\n",
    "whatsapp_file_path = 'path/to/whatsapp/messages.txt'  # Replace with actual file path of the person checking\n",
    "\n",
    "# Load the WhatsApp messages\n",
    "with open(whatsapp_file_path, 'r', encoding='utf-8') as file:\n",
    "    whatsapp_text = file.read()\n",
    "\n",
    "# Tokenization, Lemmatization, and Stemming of the WhatsApp messages\n",
    "whatsapp_tokens = word_tokenize(whatsapp_text)\n",
    "whatsapp_lemmas = [nltk_lemmatizer.lemmatize(token) for token in whatsapp_tokens]\n",
    "whatsapp_stems = [nltk_stemmer.stem(token) for token in whatsapp_tokens]\n",
    "\n",
    "# Print word statistics\n",
    "whatsapp_word_freq = Counter(whatsapp_tokens)\n",
    "print(f\"WhatsApp Messages Word Count: {len(whatsapp_tokens)}\")\n",
    "print(f\"WhatsApp Messages Most Common Words: {whatsapp_word_freq.most_common(5)}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
