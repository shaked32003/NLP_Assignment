{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b3791ff",
   "metadata": {},
   "source": [
    "# NLP Assignment\n",
    "This notebook provides hands-on practice with text processing techniques in Python, including tokenization, lemmatization, and stemming. It also includes loading, analyzing, and scraping textual data from different sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320bbad7",
   "metadata": {},
   "source": [
    "## Setup\n",
    "1. Install a Python programming environment (e.g., PyCharm, Jupyter Notebook).\n",
    "2. Install the necessary Python libraries: `nltk`, `spaCy`, `BeautifulSoup`.\n",
    "\n",
    "```python\n",
    "!pip install nltk spacy beautifulsoup4\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import spacy\n",
    "spacy.cli.download('en_core_web_sm')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83e47a",
   "metadata": {},
   "source": [
    "## Data Loading & Basic Analysis\n",
    "First, let's load the dataset and print the required basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536fbac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "# Load the spam.csv dataset\n",
    "file_path = 'spam.csv'  # Replace with your actual file path\n",
    "spam_df = pd.read_csv(file_path, encoding='latin-1')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "spam_df = spam_df[['v1', 'v2']]\n",
    "spam_df.columns = ['label', 'message']\n",
    "\n",
    "# Total number of SMS messages\n",
    "total_messages = len(spam_df)\n",
    "\n",
    "# Number of spam/ham messages\n",
    "spam_messages = len(spam_df[spam_df['label'] == 'spam'])\n",
    "ham_messages = len(spam_df[spam_df['label'] == 'ham'])\n",
    "\n",
    "# Average number of words per message\n",
    "spam_df['word_count'] = spam_df['message'].apply(lambda x: len(word_tokenize(x)))\n",
    "average_word_count = spam_df['word_count'].mean()\n",
    "\n",
    "# Most frequent words\n",
    "all_words = ' '.join(spam_df['message']).lower()\n",
    "all_words_tokenized = word_tokenize(all_words)\n",
    "word_freq = Counter(all_words_tokenized)\n",
    "most_common_words = word_freq.most_common(5)\n",
    "\n",
    "# Number of words that only appear once\n",
    "unique_words_count = sum(1 for word, count in word_freq.items() if count == 1)\n",
    "\n",
    "# Print the statistics\n",
    "print(f\"Total number of SMS messages: {total_messages}\")\n",
    "print(f\"Number of spam messages: {spam_messages}\")\n",
    "print(f\"Number of ham messages: {ham_messages}\")\n",
    "print(f\"Average number of words per message: {average_word_count}\")\n",
    "print(f\"5 most frequent words: {most_common_words}\")\n",
    "print(f\"Number of words that only appear once: {unique_words_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef11dd",
   "metadata": {},
   "source": [
    "## Text Processing\n",
    "Now, let's perform tokenization, lemmatization, and stemming using both `nltk` and `spaCy`.\n",
    "\n",
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45973fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for text processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from time import time\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# NLTK Tokenization\n",
    "start_time = time()\n",
    "spam_df['nltk_tokens'] = spam_df['message'].apply(word_tokenize)\n",
    "nltk_tokenization_time = time() - start_time\n",
    "\n",
    "# spaCy Tokenization\n",
    "start_time = time()\n",
    "spam_df['spacy_tokens'] = spam_df['message'].apply(lambda x: [token.text for token in nlp(x)])\n",
    "spacy_tokenization_time = time() - start_time\n",
    "\n",
    "# Compare tokenization times\n",
    "print(f\"NLTK Tokenization Time: {nltk_tokenization_time} seconds\")\n",
    "print(f\"spaCy Tokenization Time: {spacy_tokenization_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de6e64",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6776637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize lemmatizers\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# NLTK Lemmatization\n",
    "start_time = time()\n",
    "spam_df['nltk_lemmas'] = spam_df['nltk_tokens'].apply(lambda tokens: [nltk_lemmatizer.lemmatize(token) for token in tokens])\n",
    "nltk_lemmatization_time = time() - start_time\n",
    "\n",
    "# spaCy Lemmatization\n",
    "start_time = time()\n",
    "spam_df['spacy_lemmas'] = spam_df['spacy_tokens'].apply(lambda tokens: [token.lemma_ for token in nlp(' '.join(tokens))])\n",
    "spacy_lemmatization_time = time() - start_time\n",
    "\n",
    "# Compare lemmatization times\n",
    "print(f\"NLTK Lemmatization Time: {nltk_lemmatization_time} seconds\")\n",
    "print(f\"spaCy Lemmatization Time: {spacy_lemmatization_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17379c24",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b3da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize stemmers\n",
    "nltk_stemmer = PorterStemmer()\n",
    "\n",
    "# NLTK Stemming\n",
    "start_time = time()\n",
    "spam_df['nltk_stems'] = spam_df['nltk_tokens'].apply(lambda tokens: [nltk_stemmer.stem(token) for token in tokens])\n",
    "nltk_stemming_time = time() - start_time\n",
    "\n",
    "# Compare stemming times\n",
    "print(f\"NLTK Stemming Time: {nltk_stemming_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14461a87",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a7082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of nltk and spaCy implementations\n",
    "\n",
    "print(\"Tokenization Comparison:\")\n",
    "print(\"NLTK produces tokens as a list of strings while spaCy produces tokens as token objects with rich attributes.\")\n",
    "\n",
    "print(\"Lemmatization Comparison:\")\n",
    "print(\"NLTK lemmatizer is simpler and language-agnostic but less powerful than spaCy's lemmatizer which handles context and part-of-speech better.\")\n",
    "\n",
    "print(\"Stemming Comparison:\")\n",
    "print(\"NLTK has a built-in stemmer, whereas spaCy does not include stemming as it focuses on more advanced NLP features.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab52005e",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333922ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping using BeautifulSoup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of a public profile \n",
    "url = 'https://example.com/profile'  # Replace with The url of the person checking\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract text content from the page\n",
    "profile_text = soup.get_text()\n",
    "\n",
    "# Tokenization, Lemmatization, and Stemming of the scraped text\n",
    "profile_tokens = word_tokenize(profile_text)\n",
    "profile_lemmas = [nltk_lemmatizer.lemmatize(token) for token in profile_tokens]\n",
    "profile_stems = [nltk_stemmer.stem(token) for token in profile_tokens]\n",
    "\n",
    "# Print word statistics\n",
    "profile_word_freq = Counter(profile_tokens)\n",
    "print(f\"Profile Text Word Count: {len(profile_tokens)}\")\n",
    "print(f\"Profile Text Most Common Words: {profile_word_freq.most_common(5)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349cbb88",
   "metadata": {},
   "source": [
    "## WhatsApp Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944bf5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a .txt file of WhatsApp messages\n",
    "whatsapp_file_path = 'path/to/whatsapp/messages.txt'  # Replace with actual file path of the person checking\n",
    "\n",
    "# Load the WhatsApp messages\n",
    "with open(whatsapp_file_path, 'r', encoding='utf-8') as file:\n",
    "    whatsapp_text = file.read()\n",
    "\n",
    "# Tokenization, Lemmatization, and Stemming of the WhatsApp messages\n",
    "whatsapp_tokens = word_tokenize(whatsapp_text)\n",
    "whatsapp_lemmas = [nltk_lemmatizer.lemmatize(token) for token in whatsapp_tokens]\n",
    "whatsapp_stems = [nltk_stemmer.stem(token) for token in whatsapp_tokens]\n",
    "\n",
    "# Print word statistics\n",
    "whatsapp_word_freq = Counter(whatsapp_tokens)\n",
    "print(f\"WhatsApp Messages Word Count: {len(whatsapp_tokens)}\")\n",
    "print(f\"WhatsApp Messages Most Common Words: {whatsapp_word_freq.most_common(5)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5064e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from nltk.grammar import CFG\n",
    "from nltk.parse.chart import ChartParser\n",
    "import ace_tools as tools\n",
    "\n",
    "# Load the file content\n",
    "file_path = '/mnt/data/NLP_Assignment.ipynb'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    notebook = nbformat.read(file, as_version=4)\n",
    "\n",
    "# Extracting text cells from the notebook\n",
    "text_data = []\n",
    "for cell in notebook.cells:\n",
    "    if cell.cell_type == 'markdown' or cell.cell_type == 'code':\n",
    "        text_data.append(cell.source)\n",
    "\n",
    "# Join all text data into a single corpus\n",
    "corpus = '\\n'.join(text_data)\n",
    "\n",
    "# White space tokenizer\n",
    "white_space_tokens = corpus.split()\n",
    "\n",
    "# Regex tokenizer\n",
    "regex_tokens = re.findall(r'\\b\\w+\\b', corpus)\n",
    "\n",
    "# Simplified word tokenizer (splitting by space and punctuation)\n",
    "word_tokens = re.findall(r'\\b\\w+\\b', corpus)\n",
    "\n",
    "# Sentence tokenizer (splitting by period)\n",
    "sentences = corpus.split('.')\n",
    "\n",
    "# Stemming (using PorterStemmer-like functionality)\n",
    "stemmed_words = [re.sub(r'(ing|ed|s)$', '', word) for word in word_tokens]\n",
    "\n",
    "# Lemmatization (using a simple rule-based approach)\n",
    "lemmatized_words = [word[:-1] if word.endswith('s') else word for word in word_tokens]\n",
    "\n",
    "# Remove stop words (manually defining a list of common stop words)\n",
    "stop_words = {'and', 'or', 'but', 'if', 'while', 'with', 'a', 'an', 'the'}\n",
    "filtered_words = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Bag of Words (BOW)\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(sentences)\n",
    "bow_array = X_bow.toarray()\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf_array = X_tfidf.toarray()\n",
    "\n",
    "# Word Embedding by Word2Vec\n",
    "word2vec_model = Word2Vec([word_tokens], vector_size=100, window=5, min_count=1, workers=4)\n",
    "word2vec_similar = word2vec_model.wv.most_similar('text', topn=5)\n",
    "\n",
    "# GloVe model\n",
    "glove_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "glove_similar = glove_vectors.most_similar('text', topn=5)\n",
    "\n",
    "# Select 5 sentences for CYK tagging (dummy implementation)\n",
    "selected_sentences = [\"John saw the man with the telescope.\", \"Mary walked in the park.\", \"The dog ate a cat.\", \"A man saw a dog.\", \"Bob walked by the park.\"]\n",
    "\n",
    "# Define a simple CFG\n",
    "cfg_expanded = CFG.fromstring(\\\"\"\n",
    "    S -> NP VP\n",
    "    VP -> V NP | V NP PP | V PP\n",
    "    PP -> P NP\n",
    "    V -> \"saw\" | \"ate\" | \"walked\" | \"is\"\n",
    "    NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP | \"man\" | \"dog\" | \"cat\" | \"park\" | \"telescope\"\n",
    "    Det -> \"a\" | \"an\" | \"the\" | \"my\" | \"A\" | \"The\"\n",
    "    N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
    "    P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
    "\\\"\\\"\")\n",
    "\n",
    "# Create a chart parser\n",
    "parser_expanded = ChartParser(cfg_expanded)\n",
    "\n",
    "# Remove punctuation from sentences\n",
    "cleaned_sentences = [''.join([char for char in sentence if char not in string.punctuation]) for sentence in selected_sentences]\n",
    "\n",
    "# Parse the cleaned sentences with expanded CFG\n",
    "parsed_sentences_expanded = []\n",
    "for sentence in cleaned_sentences:\n",
    "    tokens = sentence.split()\n",
    "    parse_tree = list(parser_expanded.parse(tokens))\n",
    "    parsed_sentences_expanded.append(parse_tree)\n",
    "\n",
    "# Display results\n",
    "tools.display_dataframe_to_user(\"Tokenization and Normalization Results\", pd.DataFrame({\n",
    "    \"White Space Tokens\": white_space_tokens[:20],\n",
    "    \"Regex Tokens\": regex_tokens[:20],\n",
    "    \"Word Tokens\": word_tokens[:20],\n",
    "    \"Sentences\": sentences[:5],\n",
    "    \"Stemmed Words\": stemmed_words[:20],\n",
    "    \"Lemmatized Words\": lemmatized_words[:20],\n",
    "    \"Filtered Words\": filtered_words[:20]\n",
    "}))\n",
    "\n",
    "# Prepare results for feature extraction\n",
    "{\n",
    "    \"BOW\": bow_array,\n",
    "    \"TF-IDF\": tfidf_array,\n",
    "    \"Word2Vec Similar\": word2vec_similar,\n",
    "    \"GloVe Similar\": glove_similar,\n",
    "    \"Selected Sentences\": selected_sentences\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
