{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b3791ff",
   "metadata": {},
   "source": [
    "# NLP Assignment\n",
    "This notebook provides hands-on practice with text processing techniques in Python, including tokenization, lemmatization, and stemming. It also includes loading, analyzing, and scraping textual data from different sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320bbad7",
   "metadata": {},
   "source": [
    "## Setup\n",
    "1. Install a Python programming environment (e.g., PyCharm, Jupyter Notebook).\n",
    "2. Install the necessary Python libraries: `nltk`, `spaCy`, `BeautifulSoup`.\n",
    "\n",
    "```python\n",
    "!pip install nltk spacy beautifulsoup4\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import spacy\n",
    "spacy.cli.download('en_core_web_sm')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83e47a",
   "metadata": {},
   "source": [
    "## Data Loading & Basic Analysis\n",
    "First, let's load the dataset and print the required basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536fbac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "# Load the spam.csv dataset\n",
    "file_path = 'spam.csv'  # Replace with your actual file path\n",
    "spam_df = pd.read_csv(file_path, encoding='latin-1')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "spam_df = spam_df[['v1', 'v2']]\n",
    "spam_df.columns = ['label', 'message']\n",
    "\n",
    "# Total number of SMS messages\n",
    "total_messages = len(spam_df)\n",
    "\n",
    "# Number of spam/ham messages\n",
    "spam_messages = len(spam_df[spam_df['label'] == 'spam'])\n",
    "ham_messages = len(spam_df[spam_df['label'] == 'ham'])\n",
    "\n",
    "# Average number of words per message\n",
    "spam_df['word_count'] = spam_df['message'].apply(lambda x: len(word_tokenize(x)))\n",
    "average_word_count = spam_df['word_count'].mean()\n",
    "\n",
    "# Most frequent words\n",
    "all_words = ' '.join(spam_df['message']).lower()\n",
    "all_words_tokenized = word_tokenize(all_words)\n",
    "word_freq = Counter(all_words_tokenized)\n",
    "most_common_words = word_freq.most_common(5)\n",
    "\n",
    "# Number of words that only appear once\n",
    "unique_words_count = sum(1 for word, count in word_freq.items() if count == 1)\n",
    "\n",
    "# Print the statistics\n",
    "print(f\"Total number of SMS messages: {total_messages}\")\n",
    "print(f\"Number of spam messages: {spam_messages}\")\n",
    "print(f\"Number of ham messages: {ham_messages}\")\n",
    "print(f\"Average number of words per message: {average_word_count}\")\n",
    "print(f\"5 most frequent words: {most_common_words}\")\n",
    "print(f\"Number of words that only appear once: {unique_words_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef11dd",
   "metadata": {},
   "source": [
    "## Text Processing\n",
    "Now, let's perform tokenization, lemmatization, and stemming using both `nltk` and `spaCy`.\n",
    "\n",
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45973fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for text processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from time import time\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# NLTK Tokenization\n",
    "start_time = time()\n",
    "spam_df['nltk_tokens'] = spam_df['message'].apply(word_tokenize)\n",
    "nltk_tokenization_time = time() - start_time\n",
    "\n",
    "# spaCy Tokenization\n",
    "start_time = time()\n",
    "spam_df['spacy_tokens'] = spam_df['message'].apply(lambda x: [token.text for token in nlp(x)])\n",
    "spacy_tokenization_time = time() - start_time\n",
    "\n",
    "# Compare tokenization times\n",
    "print(f\"NLTK Tokenization Time: {nltk_tokenization_time} seconds\")\n",
    "print(f\"spaCy Tokenization Time: {spacy_tokenization_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de6e64",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6776637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize lemmatizers\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# NLTK Lemmatization\n",
    "start_time = time()\n",
    "spam_df['nltk_lemmas'] = spam_df['nltk_tokens'].apply(lambda tokens: [nltk_lemmatizer.lemmatize(token) for token in tokens])\n",
    "nltk_lemmatization_time = time() - start_time\n",
    "\n",
    "# spaCy Lemmatization\n",
    "start_time = time()\n",
    "spam_df['spacy_lemmas'] = spam_df['spacy_tokens'].apply(lambda tokens: [token.lemma_ for token in nlp(' '.join(tokens))])\n",
    "spacy_lemmatization_time = time() - start_time\n",
    "\n",
    "# Compare lemmatization times\n",
    "print(f\"NLTK Lemmatization Time: {nltk_lemmatization_time} seconds\")\n",
    "print(f\"spaCy Lemmatization Time: {spacy_lemmatization_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17379c24",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b3da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize stemmers\n",
    "nltk_stemmer = PorterStemmer()\n",
    "\n",
    "# NLTK Stemming\n",
    "start_time = time()\n",
    "spam_df['nltk_stems'] = spam_df['nltk_tokens'].apply(lambda tokens: [nltk_stemmer.stem(token) for token in tokens])\n",
    "nltk_stemming_time = time() - start_time\n",
    "\n",
    "# Compare stemming times\n",
    "print(f\"NLTK Stemming Time: {nltk_stemming_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14461a87",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a7082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of nltk and spaCy implementations\n",
    "\n",
    "print(\"Tokenization Comparison:\")\n",
    "print(\"NLTK produces tokens as a list of strings while spaCy produces tokens as token objects with rich attributes.\")\n",
    "\n",
    "print(\"Lemmatization Comparison:\")\n",
    "print(\"NLTK lemmatizer is simpler and language-agnostic but less powerful than spaCy's lemmatizer which handles context and part-of-speech better.\")\n",
    "\n",
    "print(\"Stemming Comparison:\")\n",
    "print(\"NLTK has a built-in stemmer, whereas spaCy does not include stemming as it focuses on more advanced NLP features.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab52005e",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333922ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping using BeautifulSoup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of a public profile \n",
    "url = 'https://example.com/profile'  # Replace with The url of the person checking\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract text content from the page\n",
    "profile_text = soup.get_text()\n",
    "\n",
    "# Tokenization, Lemmatization, and Stemming of the scraped text\n",
    "profile_tokens = word_tokenize(profile_text)\n",
    "profile_lemmas = [nltk_lemmatizer.lemmatize(token) for token in profile_tokens]\n",
    "profile_stems = [nltk_stemmer.stem(token) for token in profile_tokens]\n",
    "\n",
    "# Print word statistics\n",
    "profile_word_freq = Counter(profile_tokens)\n",
    "print(f\"Profile Text Word Count: {len(profile_tokens)}\")\n",
    "print(f\"Profile Text Most Common Words: {profile_word_freq.most_common(5)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349cbb88",
   "metadata": {},
   "source": [
    "## WhatsApp Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944bf5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a .txt file of WhatsApp messages\n",
    "whatsapp_file_path = 'path/to/whatsapp/messages.txt'  # Replace with actual file path of the person checking\n",
    "\n",
    "# Load the WhatsApp messages\n",
    "with open(whatsapp_file_path, 'r', encoding='utf-8') as file:\n",
    "    whatsapp_text = file.read()\n",
    "\n",
    "# Tokenization, Lemmatization, and Stemming of the WhatsApp messages\n",
    "whatsapp_tokens = word_tokenize(whatsapp_text)\n",
    "whatsapp_lemmas = [nltk_lemmatizer.lemmatize(token) for token in whatsapp_tokens]\n",
    "whatsapp_stems = [nltk_stemmer.stem(token) for token in whatsapp_tokens]\n",
    "\n",
    "# Print word statistics\n",
    "whatsapp_word_freq = Counter(whatsapp_tokens)\n",
    "print(f\"WhatsApp Messages Word Count: {len(whatsapp_tokens)}\")\n",
    "print(f\"WhatsApp Messages Most Common Words: {whatsapp_word_freq.most_common(5)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1d54d6",
   "metadata": {},
   "source": [
    "\n",
    "## Tokenization\n",
    "In this section, we will apply different tokenization methods including white space tokenizer, regex tokenizer, word tokenizer, and sentence tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ca758",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenization Section\n",
    "\n",
    "# White Space Tokenizer\n",
    "def whitespace_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "spam_df['whitespace_tokens'] = spam_df['message'].apply(whitespace_tokenizer)\n",
    "\n",
    "# Regex Tokenizer\n",
    "import re\n",
    "\n",
    "def regex_tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "spam_df['regex_tokens'] = spam_df['message'].apply(regex_tokenizer)\n",
    "\n",
    "# Word Tokenizer (NLTK)\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "spam_df['word_tokens'] = spam_df['message'].apply(word_tokenize)\n",
    "\n",
    "# Sentence Tokenizer (NLTK)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "spam_df['sentence_tokens'] = spam_df['message'].apply(sent_tokenize)\n",
    "\n",
    "# Display tokenized results\n",
    "print(spam_df[['message', 'whitespace_tokens', 'regex_tokens', 'word_tokens', 'sentence_tokens']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3accc0c4",
   "metadata": {},
   "source": [
    "\n",
    "## Normalization\n",
    "In this section, we will apply normalization techniques including stemming and lemmatization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac0d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalization Section\n",
    "\n",
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "spam_df['stemmed_tokens'] = spam_df['word_tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "\n",
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "spam_df['lemmatized_tokens'] = spam_df['word_tokens'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "\n",
    "# Display normalization results\n",
    "print(spam_df[['message', 'stemmed_tokens', 'lemmatized_tokens']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4090e7a1",
   "metadata": {},
   "source": [
    "\n",
    "## Stop Words Removal\n",
    "In this section, we will remove stop words including conjunctions and articles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084b1434",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stop Words Removal Section\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "spam_df['tokens_no_stopwords'] = spam_df['word_tokens'].apply(remove_stopwords)\n",
    "\n",
    "# Display results after stop words removal\n",
    "print(spam_df[['message', 'tokens_no_stopwords']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e652d",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Extraction\n",
    "In this section, we will apply feature extraction techniques including Bag of Words (BOW), TF-IDF, and Word2Vec.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab56e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature Extraction Section\n",
    "\n",
    "# Bag of Words (BOW)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(spam_df['message'])\n",
    "\n",
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(spam_df['message'])\n",
    "\n",
    "# Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [word_tokenize(message) for message in spam_df['message']]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "word2vec_vectors = word2vec_model.wv\n",
    "\n",
    "# Display feature extraction results\n",
    "print(\"Bag of Words shape:\", X_bow.shape)\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)\n",
    "print(\"Word2Vec vocab size:\", len(word2vec_vectors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77105563",
   "metadata": {},
   "source": [
    "\n",
    "## GloVe Explanation and Application\n",
    "GloVe (Global Vectors for Word Representation) is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "\n",
    "To apply GloVe to our data, we need to download pre-trained GloVe embeddings and map our words to these vectors. Since GloVe embeddings are pre-trained, they offer richer semantic meaning than training a Word2Vec model on a small corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fbf448",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_embeddings = {}\n",
    "with open(\"glove.6B.100d.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "        glove_embeddings[word] = vector\n",
    "\n",
    "# Function to get GloVe vector for a word\n",
    "def get_glove_vector(word):\n",
    "    return glove_embeddings.get(word, np.zeros(100))\n",
    "\n",
    "# Applying GloVe to our dataset\n",
    "spam_df['glove_vectors'] = spam_df['tokens_no_stopwords'].apply(lambda tokens: [get_glove_vector(token) for token in tokens])\n",
    "\n",
    "# Display GloVe vector results\n",
    "print(spam_df[['message', 'glove_vectors']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae8e47",
   "metadata": {},
   "source": [
    "\n",
    "## CYK Tagging\n",
    "In this section, we will select 5 sentences and apply tagging using the CYK algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f0bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CYK Tagging Section\n",
    "# Select 5 sentences for CYK tagging\n",
    "sample_sentences = spam_df['message'].sample(5).tolist()\n",
    "\n",
    "# Example CYK tagging (simplified version)\n",
    "# Here we use a dummy grammar for demonstration; replace with an actual grammar for real use cases\n",
    "grammar = {\n",
    "    'S': [['NP', 'VP']],\n",
    "    'NP': [['Det', 'N']],\n",
    "    'VP': [['V', 'NP']],\n",
    "    'Det': ['a', 'the'],\n",
    "    'N': ['dog', 'cat'],\n",
    "    'V': ['chased', 'sat']\n",
    "}\n",
    "\n",
    "def cyk_parse(sentence, grammar):\n",
    "    words = sentence.split()\n",
    "    n = len(words)\n",
    "    table = [[set() for _ in range(n)] for _ in range(n)]\n",
    "    \n",
    "    for j in range(n):\n",
    "        for lhs, rhs in grammar.items():\n",
    "            if words[j] in rhs:\n",
    "                table[j][j].add(lhs)\n",
    "        for i in range(j-1, -1, -1):\n",
    "            for k in range(i, j):\n",
    "                for lhs, rhs in grammar.items():\n",
    "                    if len(rhs) == 2 and rhs[0] in table[i][k] and rhs[1] in table[k+1][j]:\n",
    "                        table[i][j].add(lhs)\n",
    "    \n",
    "    return table\n",
    "\n",
    "# Apply CYK to sample sentences\n",
    "cyk_results = [cyk_parse(sentence, grammar) for sentence in sample_sentences]\n",
    "\n",
    "# Display CYK results\n",
    "for sentence, result in zip(sample_sentences, cyk_results):\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"CYK Parse Table: {result}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
